# RL Metaphysics

Using RL terminology as a simple practically minded foundation for metaphysics.

Let's back up first and define some (possibly deeply metaphysical) concepts made bite-sized using the terminology for decision processes (eg. featured in reinforcement learning) to simplify the concepts we'll be dealing with: *you* are an agent in an environment (aka. "reality"). The environment produces observations (the sum total of all the sensory data you experience), and *you* take actions based on the observations *you* receive. Implicit in this model is that the actions you choose affect the future observations you make, in other words, there is some causal link (it can be very weak) between what we do and what sensory experiences we make afterward. For now, let's avoid the cavernous metaphysical questions this model raises (including free will, rationalism vs empiricism, definitions of self, information theory, the directionality of time, etc), and just see where this greatly simplified model gets us. ~~(Those with an even very passing familiarity with reinforcement learning may point out that RL also features the concept of a "reward" quite prominently. It turns out we don't *ever* have to define a specific reward function).~~

Practically speaking, it would appear that you can process our observations in many different ways and depending on how we process the information we might choose different responses (let's not dwell on the implications to "free will"). Let's just call each of these ways of making a decision "model." So a model has two parts: (1) the "input" data from our raw sensory experience and (2) the way that data is processed and transformed into an action/decision. An analogy that works quite well: a model is like a function. The function alone doesn't do anything, however, if you also give the function some input data (eg. our observations), then we can apply the function to get an output.

Now let's haphazardly address another important reinforcement learning concept not yet mentioned: reward. The kind of reward function we will deal with is any function of the observation (and possibly the action). It turns out, this reward can be essentially anything you choose, as long as the reward is larger for some "observations" than others - a less formal way of saying that: you have to prefer some sensory experiences over other, but none of this argument depends on how you choose your preferences. *You* can choose whatever reward function you want. 

The only assumption you have to accept is that no matter what reward you choose, it is easier to maximize that reward if you can predict how your actions affect your future observations, and the better your predictions are the easier the optimization is. (Implicitly you also have to accept that we *should* maximize our reward, but since you can choose this reward function to be whatever you want, this is without loss of generality). In RL, this is commonly called model-based RL, where the "model" gives the agent some indication of how possible actions will help maximize the reward *before* the action is taken.

A consequence of this one assumption is that no matter how we define our reward function, improving our models will always make it easier for us to maximize our reward. This means we have effectively abstracted away the specific reward function you choose (which is why you are free to define it however you like). Now we recognize that by improving our models (aka *learning*), we can achieve whatever goal we choose.